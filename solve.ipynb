{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xF_fgC7NX1nd",
        "outputId": "d7a04727-1dde-452e-b50a-178e9028b173"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain_community in /usr/local/lib/python3.11/dist-packages (0.3.21)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.51 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.51)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.23 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.23)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (9.1.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.9.0)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.28)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.4.0)\n",
            "Requirement already satisfied: numpy<3,>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.4.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.19.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.23->langchain_community) (0.3.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.23->langchain_community) (2.11.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain_community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain_community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain_community) (4.13.1)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (3.10.16)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.23.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.1.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2025.1.31)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.51->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.23->langchain_community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.23->langchain_community) (2.33.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain_community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPq3gdYgYSAL",
        "outputId": "20a1f4d2-a9ee-4ed1-f1b6-d51541024d70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: chromadb in /usr/local/lib/python3.11/dist-packages (1.0.5)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.2.2.post1)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (2.11.3)\n",
            "Requirement already satisfied: chroma-hnswlib==0.7.6 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.7.6)\n",
            "Requirement already satisfied: fastapi==0.115.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.115.9)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.34.1)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.11/dist-packages (from chromadb) (2.0.2)\n",
            "Requirement already satisfied: posthog>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (3.25.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.13.1)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.21.1)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.32.1)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.32.1)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.53b1)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.32.1)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.21.1)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.48.9)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.67.1)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.71.0)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.3.0)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.15.2)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (32.0.1)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (9.1.2)\n",
            "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.0.2)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (5.1.0)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb) (3.10.16)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.23.0)\n",
            "Requirement already satisfied: starlette<0.46.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi==0.115.9->chromadb) (0.45.3)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (24.2)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.24.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.32.3)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.3.0)\n",
            "Requirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (0.9)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.18)\n",
            "Requirement already satisfied: importlib-metadata<8.7.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.6.1)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.69.2)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.32.1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.32.1)\n",
            "Requirement already satisfied: opentelemetry-proto==1.32.1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.32.1)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.53b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.53b1)\n",
            "Requirement already satisfied: opentelemetry-instrumentation==0.53b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.53b1)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.53b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.53b1)\n",
            "Requirement already satisfied: opentelemetry-util-http==0.53b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.53b1)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation==0.53b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.17.2)\n",
            "Requirement already satisfied: asgiref~=3.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-asgi==0.53b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.1)\n",
            "Requirement already satisfied: monotonic>=1.5 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (0.4.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (2.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers>=0.13.2->chromadb) (0.30.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.0)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.5)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.3.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.21.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kubernetes>=28.1.0->chromadb) (3.4.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install chromadb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z57Ej5vAXnpM",
        "outputId": "1047de81-fa17-4125-f8b8-3e70f3f4e0c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting STRUCTURED full pull request data fetch for AlfaInsurance/devQ_testData_PythonProject\n",
            "Target PR state: all\n",
            "Output directory: pull_request_data_structured\n",
            "Ensure GITHUB_BOT_ACCESS_TOKEN environment variable is set.\n",
            "WARNING: This can take a long time and consume significant disk space and API calls.\n",
            "--- Starting STRUCTURED pull request data fetch for AlfaInsurance/devQ_testData_PythonProject ---\n",
            "--- Output base directory: pull_request_data_structured ---\n",
            "\n",
            "Fetching page 1 of pull requests list from https://api.github.com/repos/AlfaInsurance/devQ_testData_PythonProject/pulls...\n",
            "Processing 2 pull requests from page 1...\n",
            "\n",
            "--- Processing PR #1: Hackaton ---\n",
            "    Updated at: 2025-04-14T09:00:47Z\n",
            "    Fetching full details for PR #1...\n",
            "    Fetching changed files for PR #1...\n",
            "      Fetching page 1 from https://api.github.com/repos/AlfaInsurance/devQ_testData_PythonProject/pulls/1/files...\n",
            "    Processing 16 files for PR #1...\n",
            "      Processing file: .gitlab-ci.yml (Status: removed)\n",
            "        Fetching content: .gitlab-ci.yml @ c916a6d\n",
            "      Processing file: accesslist/templates/acl_create_info.html (Status: modified)\n",
            "        Fetching content: accesslist/templates/acl_create_info.html @ c916a6d\n",
            "        Fetching content: accesslist/templates/acl_create_info.html @ 28b7d14\n",
            "      Processing file: accesslist/templates/acl_demo.html (Status: modified)\n",
            "        Fetching content: accesslist/templates/acl_demo.html @ c916a6d\n",
            "        Fetching content: accesslist/templates/acl_demo.html @ 28b7d14\n",
            "      Processing file: accesslist/templates/acl_dmz_resources.html (Status: modified)\n",
            "        Fetching content: accesslist/templates/acl_dmz_resources.html @ c916a6d\n",
            "        Fetching content: accesslist/templates/acl_dmz_resources.html @ 28b7d14\n",
            "      Processing file: accesslist/utils/gitlab.py (Status: modified)\n",
            "        Fetching content: accesslist/utils/gitlab.py @ c916a6d\n",
            "HTTP error making API request to https://api.github.com/repos/AlfaInsurance/devQ_testData_PythonProject/contents/accesslist/utils/gitlab.py?ref=c916a6d807e86e2f88a38d1f960dedae53d8e97e: 404 Client Error: Not Found for url: https://api.github.com/repos/AlfaInsurance/devQ_testData_PythonProject/contents/accesslist/utils/gitlab.py?ref=c916a6d807e86e2f88a38d1f960dedae53d8e97e\n",
            "Response Status: 404\n",
            "Response Body: {\"message\":\"Not Found\",\"documentation_url\":\"https://docs.github.com/rest/repos/contents#get-repository-content\",\"status\":\"404\"}\n",
            "        Fetching content: accesslist/utils/gitlab.py @ 28b7d14\n",
            "      Processing file: accesslist/views.py (Status: modified)\n",
            "        Fetching content: accesslist/views.py @ c916a6d\n",
            "        Fetching content: accesslist/views.py @ 28b7d14\n",
            "      Processing file: acladmin/settings.py (Status: modified)\n",
            "        Fetching content: acladmin/settings.py @ c916a6d\n",
            "        Fetching content: acladmin/settings.py @ 28b7d14\n",
            "      Processing file: acladmin/tasks.py (Status: modified)\n",
            "        Fetching content: acladmin/tasks.py @ c916a6d\n",
            "        Fetching content: acladmin/tasks.py @ 28b7d14\n",
            "      Processing file: acladmin/wsgi.py (Status: modified)\n",
            "        Fetching content: acladmin/wsgi.py @ c916a6d\n",
            "        Fetching content: acladmin/wsgi.py @ 28b7d14\n",
            "      Processing file: init.sh (Status: removed)\n",
            "        Fetching content: init.sh @ c916a6d\n",
            "HTTP error making API request to https://api.github.com/repos/AlfaInsurance/devQ_testData_PythonProject/contents/init.sh?ref=c916a6d807e86e2f88a38d1f960dedae53d8e97e: 404 Client Error: Not Found for url: https://api.github.com/repos/AlfaInsurance/devQ_testData_PythonProject/contents/init.sh?ref=c916a6d807e86e2f88a38d1f960dedae53d8e97e\n",
            "Response Status: 404\n",
            "Response Body: {\"message\":\"Not Found\",\"documentation_url\":\"https://docs.github.com/rest/repos/contents#get-repository-content\",\"status\":\"404\"}\n",
            "      Processing file: ownerlist/templates/registration/login.html (Status: modified)\n",
            "        Fetching content: ownerlist/templates/registration/login.html @ c916a6d\n",
            "        Fetching content: ownerlist/templates/registration/login.html @ 28b7d14\n",
            "      Processing file: ownerlist/utils.py (Status: modified)\n",
            "        Fetching content: ownerlist/utils.py @ c916a6d\n",
            "        Fetching content: ownerlist/utils.py @ 28b7d14\n",
            "      Processing file: panel/templates/panel.html (Status: modified)\n",
            "        Fetching content: panel/templates/panel.html @ c916a6d\n",
            "        Fetching content: panel/templates/panel.html @ 28b7d14\n",
            "      Processing file: panel/views.py (Status: modified)\n",
            "        Fetching content: panel/views.py @ c916a6d\n",
            "        Fetching content: panel/views.py @ 28b7d14\n",
            "      Processing file: requirements.txt (Status: modified)\n",
            "        Fetching content: requirements.txt @ c916a6d\n",
            "        Fetching content: requirements.txt @ 28b7d14\n",
            "      Processing file: templates/base.html (Status: modified)\n",
            "        Fetching content: templates/base.html @ c916a6d\n",
            "        Fetching content: templates/base.html @ 28b7d14\n",
            "    Fetching reviews for PR #1...\n",
            "      Fetching page 1 from https://api.github.com/repos/AlfaInsurance/devQ_testData_PythonProject/pulls/1/reviews...\n",
            "    Fetching review comments for PR #1...\n",
            "      Fetching page 1 from https://api.github.com/repos/AlfaInsurance/devQ_testData_PythonProject/pulls/1/comments...\n",
            "    Fetching issue comments for PR #1...\n",
            "      Fetching page 1 from https://api.github.com/repos/AlfaInsurance/devQ_testData_PythonProject/issues/1/comments...\n",
            "    Fetching commits for PR #1...\n",
            "      Fetching page 1 from https://api.github.com/repos/AlfaInsurance/devQ_testData_PythonProject/pulls/1/commits...\n",
            "      Fetching page 2 from https://api.github.com/repositories/964502314/pulls/1/commits...\n",
            "      Fetching page 3 from https://api.github.com/repositories/964502314/pulls/1/commits...\n",
            "    Fetching check runs for commit 28b7d14...\n",
            "    Fetching statuses for commit 28b7d14...\n",
            "      Fetching page 1 from https://api.github.com/repos/AlfaInsurance/devQ_testData_PythonProject/commits/28b7d14401ca39872a6922ac3df25e1ef95c0750/statuses...\n",
            "    Successfully saved metadata to pull_request_data_structured/pr_1/metadata.json\n",
            "\n",
            "--- Processing PR #2: v1 ---\n",
            "    Updated at: 2025-04-11T10:14:27Z\n",
            "    Fetching full details for PR #2...\n",
            "    Fetching changed files for PR #2...\n",
            "      Fetching page 1 from https://api.github.com/repos/AlfaInsurance/devQ_testData_PythonProject/pulls/2/files...\n",
            "    Processing 16 files for PR #2...\n",
            "      Processing file: .gitlab-ci.yml (Status: removed)\n",
            "        Fetching content: .gitlab-ci.yml @ a32789d\n",
            "      Processing file: accesslist/templates/acl_create_info.html (Status: modified)\n",
            "        Fetching content: accesslist/templates/acl_create_info.html @ a32789d\n",
            "        Fetching content: accesslist/templates/acl_create_info.html @ 28b7d14\n",
            "      Processing file: accesslist/templates/acl_demo.html (Status: modified)\n",
            "        Fetching content: accesslist/templates/acl_demo.html @ a32789d\n",
            "        Fetching content: accesslist/templates/acl_demo.html @ 28b7d14\n",
            "      Processing file: accesslist/templates/acl_dmz_resources.html (Status: modified)\n",
            "        Fetching content: accesslist/templates/acl_dmz_resources.html @ a32789d\n",
            "        Fetching content: accesslist/templates/acl_dmz_resources.html @ 28b7d14\n",
            "      Processing file: accesslist/utils/gitlab.py (Status: modified)\n",
            "        Fetching content: accesslist/utils/gitlab.py @ a32789d\n",
            "        Fetching content: accesslist/utils/gitlab.py @ 28b7d14\n",
            "      Processing file: accesslist/views.py (Status: modified)\n",
            "        Fetching content: accesslist/views.py @ a32789d\n",
            "        Fetching content: accesslist/views.py @ 28b7d14\n",
            "      Processing file: acladmin/settings.py (Status: modified)\n",
            "        Fetching content: acladmin/settings.py @ a32789d\n",
            "        Fetching content: acladmin/settings.py @ 28b7d14\n",
            "      Processing file: acladmin/tasks.py (Status: modified)\n",
            "        Fetching content: acladmin/tasks.py @ a32789d\n",
            "        Fetching content: acladmin/tasks.py @ 28b7d14\n",
            "      Processing file: acladmin/wsgi.py (Status: modified)\n",
            "        Fetching content: acladmin/wsgi.py @ a32789d\n",
            "        Fetching content: acladmin/wsgi.py @ 28b7d14\n",
            "      Processing file: init.sh (Status: removed)\n",
            "        Fetching content: init.sh @ a32789d\n",
            "      Processing file: ownerlist/templates/registration/login.html (Status: modified)\n",
            "        Fetching content: ownerlist/templates/registration/login.html @ a32789d\n",
            "        Fetching content: ownerlist/templates/registration/login.html @ 28b7d14\n",
            "      Processing file: ownerlist/utils.py (Status: modified)\n",
            "        Fetching content: ownerlist/utils.py @ a32789d\n",
            "        Fetching content: ownerlist/utils.py @ 28b7d14\n",
            "      Processing file: panel/templates/panel.html (Status: modified)\n",
            "        Fetching content: panel/templates/panel.html @ a32789d\n",
            "        Fetching content: panel/templates/panel.html @ 28b7d14\n",
            "      Processing file: panel/views.py (Status: modified)\n",
            "        Fetching content: panel/views.py @ a32789d\n",
            "        Fetching content: panel/views.py @ 28b7d14\n",
            "      Processing file: requirements.txt (Status: modified)\n",
            "        Fetching content: requirements.txt @ a32789d\n",
            "        Fetching content: requirements.txt @ 28b7d14\n",
            "      Processing file: templates/base.html (Status: modified)\n",
            "        Fetching content: templates/base.html @ a32789d\n",
            "        Fetching content: templates/base.html @ 28b7d14\n",
            "    Fetching reviews for PR #2...\n",
            "      Fetching page 1 from https://api.github.com/repos/AlfaInsurance/devQ_testData_PythonProject/pulls/2/reviews...\n",
            "    Fetching review comments for PR #2...\n",
            "      Fetching page 1 from https://api.github.com/repos/AlfaInsurance/devQ_testData_PythonProject/pulls/2/comments...\n",
            "    Fetching issue comments for PR #2...\n",
            "      Fetching page 1 from https://api.github.com/repos/AlfaInsurance/devQ_testData_PythonProject/issues/2/comments...\n",
            "    Fetching commits for PR #2...\n",
            "      Fetching page 1 from https://api.github.com/repos/AlfaInsurance/devQ_testData_PythonProject/pulls/2/commits...\n",
            "    Fetching check runs for commit 28b7d14...\n",
            "    Fetching statuses for commit 28b7d14...\n",
            "      Fetching page 1 from https://api.github.com/repos/AlfaInsurance/devQ_testData_PythonProject/commits/28b7d14401ca39872a6922ac3df25e1ef95c0750/statuses...\n",
            "    Successfully saved metadata to pull_request_data_structured/pr_2/metadata.json\n",
            "No 'next' link found in PR list response, reached the last page.\n",
            "\n",
            "--- Finished STRUCTURED processing for AlfaInsurance/devQ_testData_PythonProject. ---\n",
            "--- Processed 2 pull requests. ---\n",
            "--- Data saved in subdirectories within: pull_request_data_structured ---\n",
            "\n",
            "--------------------------------------------------\n",
            "Successfully finished processing.\n",
            "Processed 2 pull requests.\n",
            "Data saved in 'pull_request_data_structured' directory, organized by PR number.\n",
            "Total execution time: 17.17 seconds\n",
            "--------------------------------------------------\n",
            "\n",
            "--- Starting RAG Analysis ---\n",
            "Initializing embeddings model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name microsoft/graphcodebert-base. Creating a new one with mean pooling.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embeddings model initialized.\n",
            "Initializing LLM: TinyLlama/TinyLlama-1.1B-Chat-v1.0...\n",
            "Cleaning up temporary Chroma directories...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LLM initialized successfully.\n",
            "\n",
            "\u001b[1m--- Analyzing PR #1 ---\u001b[0m\n",
            "\n",
            "Вопрос: Какие потенциальные уязвимости есть в этих изменениях?\n",
            "PR #1 data not loaded. Attempting to load...\n",
            "Processing data for PR #1...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing files for PR #1: 100%|██████████| 16/16 [00:00<00:00, 252.37it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully created Chroma DB for PR #1 in temporary directory: /tmp/chroma_db_pr_1__ygkg6sb\n",
            "--- Debugging Prompt Variables for PR #1 ---\n",
            "pr_number_str: 1\n",
            "question: Какие потенциальные уязвимости есть в этих изменениях?\n",
            "ci_checks_str: No CI checks found.\n",
            "context_text (first 200 chars): def task(request, acl_id) -> bool:\n",
            "    return\n",
            "    \"\"\"Функция обработки запросов на выполнение активностей для выполнения обращения\"\"\"\n",
            "    logger.info(\n",
            "        f\"[Отправка в omni] Начинается выполнение...\n",
            "--- End Debugging Print Statements ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ответ: 1. Code changes (diff): The PR contains no code changes. Therefore, there are no potential vulnerabilities in these changes.\n",
            "\n",
            "            2. Developer comments (issue and review comments): The PR mentions no developer comments or issues related to security. Therefore, there are no potential vulnerabilities in these comments.\n",
            "\n",
            "            3. Results of CI checks: The PR does not mention any CI checks. Therefore, there are no potential vulnerabilities in these checks.\n",
            "\n",
            "            4. Commit history (summarized in context): The commit history summarized in the context shows that the PR has been merged into the main branch successfully. Therefore, there are no potential vulnerabilities in this context.\n",
            "Источники: [{'content_snippet': 'def task(request, acl_id) -> bool:\\n    return\\n    \"\"\"Функция обработки запросов на выполнение активностей для выполнения обращения\"\"\"\\n    logger.info(\\n        f\"[Отправка в omni] Начинается выполнение...', 'file': 'Unknown (metadata not stored)', 'checks': 'Unknown (metadata not stored)', 'author': 'Unknown (metadata not stored)'}, {'content_snippet': 'def task(request, acl_id) -> bool:\\n    return\\n    \"\"\"Функция обработки запросов на выполнение активностей для выполнения обращения\"\"\"\\n    logger.info(\\n        f\"[Отправка в omni] Начинается выполнение...', 'file': 'Unknown (metadata not stored)', 'checks': 'Unknown (metadata not stored)', 'author': 'Unknown (metadata not stored)'}]\n",
            "\n",
            "Вопрос: Соответствует ли код стандартам проекта?\n",
            "--- Debugging Prompt Variables for PR #1 ---\n",
            "pr_number_str: 1\n",
            "question: Соответствует ли код стандартам проекта?\n",
            "ci_checks_str: No CI checks found.\n",
            "context_text (first 200 chars): def task(request, acl_id) -> bool:\n",
            "    return\n",
            "    \"\"\"Функция обработки запросов на выполнение активностей для выполнения обращения\"\"\"\n",
            "    logger.info(\n",
            "        f\"[Отправка в omni] Начинается выполнение...\n",
            "--- End Debugging Print Statements ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ответ: Yes, the code in the given PR meets the project standards. The PR follows the established coding conventions and best practices for Python programming. It also includes appropriate comments and documentation to explain the purpose and functionality of each function or class. Additionally, the code passes all unit tests and has been thoroughly tested by the developer before submission.\n",
            "Источники: [{'content_snippet': 'def task(request, acl_id) -> bool:\\n    return\\n    \"\"\"Функция обработки запросов на выполнение активностей для выполнения обращения\"\"\"\\n    logger.info(\\n        f\"[Отправка в omni] Начинается выполнение...', 'file': 'Unknown (metadata not stored)', 'checks': 'Unknown (metadata not stored)', 'author': 'Unknown (metadata not stored)'}, {'content_snippet': 'def task(request, acl_id) -> bool:\\n    return\\n    \"\"\"Функция обработки запросов на выполнение активностей для выполнения обращения\"\"\"\\n    logger.info(\\n        f\"[Отправка в omni] Начинается выполнение...', 'file': 'Unknown (metadata not stored)', 'checks': 'Unknown (metadata not stored)', 'author': 'Unknown (metadata not stored)'}]\n",
            "\n",
            "Вопрос: Есть ли проблемы с производительностью в измененном коде?\n",
            "--- Debugging Prompt Variables for PR #1 ---\n",
            "pr_number_str: 1\n",
            "question: Есть ли проблемы с производительностью в измененном коде?\n",
            "ci_checks_str: No CI checks found.\n",
            "context_text (first 200 chars): def task(request, acl_id) -> bool:\n",
            "    return\n",
            "    \"\"\"Функция обработки запросов на выполнение активностей для выполнения обращения\"\"\"\n",
            "    logger.info(\n",
            "        f\"[Отправка в omni] Начинается выполнение...\n",
            "--- End Debugging Print Statements ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ответ: Да, есть возможность обнаружить некоторые проблемы с производительностью в измененном коде. Это связано с тем, что изменения могут привести к увеличению размера файлов и переполнению памяти, что может повлиять на производительность при выполнении программного кода. В случае, если вы заметите, что изменения в коде приводят к увеличению размера файлов или переполнению памяти, то следует проверить, какие изменения были внесены, чтобы определить, какая из них привела к проблемам. Также, можно использовать инструменты для проверки производительности, такие как `pyperf` или `cProfile`.\n",
            "Источники: [{'content_snippet': 'def task(request, acl_id) -> bool:\\n    return\\n    \"\"\"Функция обработки запросов на выполнение активностей для выполнения обращения\"\"\"\\n    logger.info(\\n        f\"[Отправка в omni] Начинается выполнение...', 'file': 'Unknown (metadata not stored)', 'checks': 'Unknown (metadata not stored)', 'author': 'Unknown (metadata not stored)'}, {'content_snippet': 'def task(request, acl_id) -> bool:\\n    return\\n    \"\"\"Функция обработки запросов на выполнение активностей для выполнения обращения\"\"\"\\n    logger.info(\\n        f\"[Отправка в omni] Начинается выполнение...', 'file': 'Unknown (metadata not stored)', 'checks': 'Unknown (metadata not stored)', 'author': 'Unknown (metadata not stored)'}]\n",
            "\n",
            "Вопрос: Summarize the main changes in PR 1.\n",
            "--- Debugging Prompt Variables for PR #1 ---\n",
            "pr_number_str: 1\n",
            "question: Summarize the main changes in PR 1.\n",
            "ci_checks_str: No CI checks found.\n",
            "context_text (first 200 chars): --- Pull Request #1 - Hackaton ---\n",
            "Author: VasilevArtem\n",
            "File: templates/base.html\n",
            "Status: closed\n",
            "CI Checks for head commit: No CI checks found.\n",
            "\n",
            "---\n",
            "\n",
            "--- Pull Request #1 - Hackaton ---\n",
            "Author: Vasilev...\n",
            "--- End Debugging Print Statements ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ответ: PR 1 made significant changes to the base template file and added a new template file with additional features. The changes include adding a new section for access control lists (ACLs), which allows users to specify which files or directories they can access. Additionally, there were no CI checks found in the pull request, indicating that the developer did not perform any automated testing or validation before submitting the PR.\n",
            "Источники: [{'content_snippet': '--- Pull Request #1 - Hackaton ---\\nAuthor: VasilevArtem\\nFile: templates/base.html\\nStatus: closed\\nCI Checks for head commit: No CI checks found....', 'file': 'Unknown (metadata not stored)', 'checks': 'Unknown (metadata not stored)', 'author': 'Unknown (metadata not stored)'}, {'content_snippet': '--- Pull Request #1 - Hackaton ---\\nAuthor: VasilevArtem\\nFile: accesslist/templates/acl_demo.html\\nStatus: closed\\nCI Checks for head commit: No CI checks found....', 'file': 'Unknown (metadata not stored)', 'checks': 'Unknown (metadata not stored)', 'author': 'Unknown (metadata not stored)'}]\n",
            "\n",
            "Вопрос: What were the CI check results for PR 2?\n",
            "--- Debugging Prompt Variables for PR #1 ---\n",
            "pr_number_str: 1\n",
            "question: What were the CI check results for PR 2?\n",
            "ci_checks_str: No CI checks found.\n",
            "context_text (first 200 chars): --- Pull Request #1 - Hackaton ---\n",
            "Author: VasilevArtem\n",
            "File: templates/base.html\n",
            "Status: closed\n",
            "CI Checks for head commit: No CI checks found.\n",
            "\n",
            "---\n",
            "\n",
            "--- Pull Request #1 - Hackaton ---\n",
            "Author: Vasilev...\n",
            "--- End Debugging Print Statements ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ответ: The CI check results for PR 2 are not provided in the given text. Please provide them yourself or refer to the given text for more details.\n",
            "Источники: [{'content_snippet': '--- Pull Request #1 - Hackaton ---\\nAuthor: VasilevArtem\\nFile: templates/base.html\\nStatus: closed\\nCI Checks for head commit: No CI checks found....', 'file': 'Unknown (metadata not stored)', 'checks': 'Unknown (metadata not stored)', 'author': 'Unknown (metadata not stored)'}, {'content_snippet': '--- Pull Request #1 - Hackaton ---\\nAuthor: VasilevArtem\\nFile: accesslist/templates/acl_demo.html\\nStatus: closed\\nCI Checks for head commit: No CI checks found....', 'file': 'Unknown (metadata not stored)', 'checks': 'Unknown (metadata not stored)', 'author': 'Unknown (metadata not stored)'}]\n",
            "\n",
            "\u001b[1m--- Analyzing PR #2 ---\u001b[0m\n",
            "\n",
            "Вопрос: Какие потенциальные уязвимости есть в этих изменениях?\n",
            "PR #2 data not loaded. Attempting to load...\n",
            "Processing data for PR #2...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing files for PR #2: 100%|██████████| 16/16 [00:00<00:00, 365.46it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully created Chroma DB for PR #2 in temporary directory: /tmp/chroma_db_pr_2_uzi57pen\n",
            "--- Debugging Prompt Variables for PR #2 ---\n",
            "pr_number_str: 2\n",
            "question: Какие потенциальные уязвимости есть в этих изменениях?\n",
            "ci_checks_str: No CI checks found.\n",
            "context_text (first 200 chars): def task(request, acl_id) -> bool:\n",
            "    return\n",
            "    \"\"\"Функция обработки запросов на выполнение активностей для выполнения обращения\"\"\"\n",
            "    logger.info(\n",
            "        f\"[Отправка в omni] Начинается выполнение...\n",
            "--- End Debugging Print Statements ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ответ: 1. Code changes (diff): The PR contains no code changes. Therefore, there are no potential vulnerabilities in these changes.\n",
            "\n",
            "            2. Developer comments (issue and review comments): The PR mentions no developer comments or issues related to security. Therefore, there are no potential vulnerabilities in these comments.\n",
            "\n",
            "            3. Results of CI checks: The PR does not mention any CI checks. Therefore, there are no potential vulnerabilities in these checks.\n",
            "\n",
            "            4. Commit history (summarized in context): The commit history summarized in the context shows that the PR has been merged into the main branch successfully. Therefore, there are no potential vulnerabilities in this context.\n",
            "Источники: [{'content_snippet': 'def task(request, acl_id) -> bool:\\n    return\\n    \"\"\"Функция обработки запросов на выполнение активностей для выполнения обращения\"\"\"\\n    logger.info(\\n        f\"[Отправка в omni] Начинается выполнение...', 'file': 'Unknown (metadata not stored)', 'checks': 'Unknown (metadata not stored)', 'author': 'Unknown (metadata not stored)'}, {'content_snippet': 'def task(request, acl_id) -> bool:\\n    return\\n    \"\"\"Функция обработки запросов на выполнение активностей для выполнения обращения\"\"\"\\n    logger.info(\\n        f\"[Отправка в omni] Начинается выполнение...', 'file': 'Unknown (metadata not stored)', 'checks': 'Unknown (metadata not stored)', 'author': 'Unknown (metadata not stored)'}]\n",
            "\n",
            "Вопрос: Соответствует ли код стандартам проекта?\n",
            "--- Debugging Prompt Variables for PR #2 ---\n",
            "pr_number_str: 2\n",
            "question: Соответствует ли код стандартам проекта?\n",
            "ci_checks_str: No CI checks found.\n",
            "context_text (first 200 chars): def task(request, acl_id) -> bool:\n",
            "    return\n",
            "    \"\"\"Функция обработки запросов на выполнение активностей для выполнения обращения\"\"\"\n",
            "    logger.info(\n",
            "        f\"[Отправка в omni] Начинается выполнение...\n",
            "--- End Debugging Print Statements ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ответ: Yes, the code in the given PR meets the project standards. The PR follows the established coding conventions and best practices for Python programming. It also includes appropriate comments and documentation to explain the purpose and functionality of each function or class. Additionally, the code passes all unit tests and has been thoroughly tested by the developer before submission.\n",
            "Источники: [{'content_snippet': 'def task(request, acl_id) -> bool:\\n    return\\n    \"\"\"Функция обработки запросов на выполнение активностей для выполнения обращения\"\"\"\\n    logger.info(\\n        f\"[Отправка в omni] Начинается выполнение...', 'file': 'Unknown (metadata not stored)', 'checks': 'Unknown (metadata not stored)', 'author': 'Unknown (metadata not stored)'}, {'content_snippet': 'def task(request, acl_id) -> bool:\\n    return\\n    \"\"\"Функция обработки запросов на выполнение активностей для выполнения обращения\"\"\"\\n    logger.info(\\n        f\"[Отправка в omni] Начинается выполнение...', 'file': 'Unknown (metadata not stored)', 'checks': 'Unknown (metadata not stored)', 'author': 'Unknown (metadata not stored)'}]\n",
            "\n",
            "Вопрос: Есть ли проблемы с производительностью в измененном коде?\n",
            "--- Debugging Prompt Variables for PR #2 ---\n",
            "pr_number_str: 2\n",
            "question: Есть ли проблемы с производительностью в измененном коде?\n",
            "ci_checks_str: No CI checks found.\n",
            "context_text (first 200 chars): def task(request, acl_id) -> bool:\n",
            "    return\n",
            "    \"\"\"Функция обработки запросов на выполнение активностей для выполнения обращения\"\"\"\n",
            "    logger.info(\n",
            "        f\"[Отправка в omni] Начинается выполнение...\n",
            "--- End Debugging Print Statements ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ответ: Да, есть возможность заметить некоторые проблемы с производительностью в измененном коде. Это может быть связано с различными причинами, такими как:\n",
            "\n",
            "1. Увеличение размера файлов или переменных, что приводит к увеличению времени загрузки и запуска программы.\n",
            "2. Использование больших массивов или списков, которые могут привести к выделению памяти и повышению требований к ресурсам.\n",
            "3. Введение новых функций или переименования существующих, что приводит к необходимости перечисления всех функций и переименованию их в более подходящих названиях.\n",
            "4. Применение новых технологий, которые могут привести к увеличению времени работы программы.\n",
            "5. Ограничение количества оперативной памяти, которое может привести к тому, что программа работает медленнее.\n",
            "\n",
            "Если вы заметите эти проблемы, то следует проверить, какие из них возникают в изменённом коде, чтобы определить, какая из них является основной причиной. После этого можно принять меры по устранению этих проблем, чтобы повысить производительность кода.\n",
            "Источники: [{'content_snippet': 'def task(request, acl_id) -> bool:\\n    return\\n    \"\"\"Функция обработки запросов на выполнение активностей для выполнения обращения\"\"\"\\n    logger.info(\\n        f\"[Отправка в omni] Начинается выполнение...', 'file': 'Unknown (metadata not stored)', 'checks': 'Unknown (metadata not stored)', 'author': 'Unknown (metadata not stored)'}, {'content_snippet': 'def task(request, acl_id) -> bool:\\n    return\\n    \"\"\"Функция обработки запросов на выполнение активностей для выполнения обращения\"\"\"\\n    logger.info(\\n        f\"[Отправка в omni] Начинается выполнение...', 'file': 'Unknown (metadata not stored)', 'checks': 'Unknown (metadata not stored)', 'author': 'Unknown (metadata not stored)'}]\n",
            "\n",
            "Вопрос: Summarize the main changes in PR 1.\n",
            "--- Debugging Prompt Variables for PR #2 ---\n",
            "pr_number_str: 2\n",
            "question: Summarize the main changes in PR 1.\n",
            "ci_checks_str: No CI checks found.\n",
            "context_text (first 200 chars): --- Pull Request #2 - v1 ---\n",
            "Author: VasilevArtem\n",
            "File: templates/base.html\n",
            "Status: closed\n",
            "CI Checks for head commit: No CI checks found.\n",
            "\n",
            "---\n",
            "\n",
            "--- Pull Request #2 - v1 ---\n",
            "Author: VasilevArtem\n",
            "File: ...\n",
            "--- End Debugging Print Statements ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ответ: PR 1 made significant changes to the base template file, including adding new variables and modifying existing ones. The author also added new HTML elements and updated existing ones to improve the overall layout and functionality of the template. Additionally, the author included developer comments and CI checks for the head commit, but these were not included in the context provided. Overall, the changes made by the author aimed to enhance the template's usability and functionality.\n",
            "Источники: [{'content_snippet': '--- Pull Request #2 - v1 ---\\nAuthor: VasilevArtem\\nFile: templates/base.html\\nStatus: closed\\nCI Checks for head commit: No CI checks found....', 'file': 'Unknown (metadata not stored)', 'checks': 'Unknown (metadata not stored)', 'author': 'Unknown (metadata not stored)'}, {'content_snippet': '--- Pull Request #2 - v1 ---\\nAuthor: VasilevArtem\\nFile: accesslist/templates/acl_demo.html\\nStatus: closed\\nCI Checks for head commit: No CI checks found....', 'file': 'Unknown (metadata not stored)', 'checks': 'Unknown (metadata not stored)', 'author': 'Unknown (metadata not stored)'}]\n",
            "\n",
            "Вопрос: What were the CI check results for PR 2?\n",
            "--- Debugging Prompt Variables for PR #2 ---\n",
            "pr_number_str: 2\n",
            "question: What were the CI check results for PR 2?\n",
            "ci_checks_str: No CI checks found.\n",
            "context_text (first 200 chars): --- Pull Request #2 - v1 ---\n",
            "Author: VasilevArtem\n",
            "File: templates/base.html\n",
            "Status: closed\n",
            "CI Checks for head commit: No CI checks found.\n",
            "\n",
            "---\n",
            "\n",
            "--- Pull Request #2 - v1 ---\n",
            "Author: VasilevArtem\n",
            "File: ...\n",
            "--- End Debugging Print Statements ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ответ: The given text doesn't provide any specific information about the CI check results for PR 2. It only mentions that there are no CI checks found for the head commit.\n",
            "Источники: [{'content_snippet': '--- Pull Request #2 - v1 ---\\nAuthor: VasilevArtem\\nFile: templates/base.html\\nStatus: closed\\nCI Checks for head commit: No CI checks found....', 'file': 'Unknown (metadata not stored)', 'checks': 'Unknown (metadata not stored)', 'author': 'Unknown (metadata not stored)'}, {'content_snippet': '--- Pull Request #2 - v1 ---\\nAuthor: VasilevArtem\\nFile: accesslist/templates/acl_demo.html\\nStatus: closed\\nCI Checks for head commit: No CI checks found....', 'file': 'Unknown (metadata not stored)', 'checks': 'Unknown (metadata not stored)', 'author': 'Unknown (metadata not stored)'}]\n",
            "\n",
            "--- RAG Analysis Finished ---\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import requests\n",
        "import json\n",
        "import base64\n",
        "import time\n",
        "import re\n",
        "from datetime import datetime\n",
        "import tempfile \n",
        "import shutil \n",
        "from langchain.text_splitter import Language, RecursiveCharacterTextSplitter\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "from tqdm import tqdm \n",
        "\n",
        "\n",
        "\n",
        "github_secret = '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'\n",
        "\n",
        "\n",
        "\n",
        "OUTPUT_DIR_BASE = \"pull_request_data_structured\"\n",
        "API_VERSION = '2022-11-28'\n",
        "PER_PAGE = 100 \n",
        "REQUEST_TIMEOUT = 60 \n",
        "\n",
        "\n",
        "def make_api_request(url, headers, params=None):\n",
        "\n",
        "    headers['X-GitHub-Api-Version'] = API_VERSION\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers, params=params, timeout=REQUEST_TIMEOUT)\n",
        "        response.raise_for_status() \n",
        "        return response\n",
        "    except requests.exceptions.Timeout:\n",
        "        print(f\"Timeout error making API request to {url}\")\n",
        "        return None\n",
        "    except requests.exceptions.HTTPError as e:\n",
        "        print(f\"HTTP error making API request to {url}: {e}\")\n",
        "        print(f\"Response Status: {e.response.status_code}\")\n",
        "        print(f\"Response Body: {e.response.text}\")\n",
        "        ''\n",
        "        ''\n",
        "        return None\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error making API request to {url}: {e}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred during API request to {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "def fetch_paginated_data(url, headers, params=None):\n",
        "\n",
        "    if params is None:\n",
        "        params = {}\n",
        "    params['per_page'] = PER_PAGE\n",
        "    all_items = []\n",
        "    current_url = url\n",
        "    page = 1\n",
        "\n",
        "    while current_url:\n",
        "         \n",
        "        cleaned_url = current_url.replace('[https://', 'https://').replace(']', '')\n",
        "        print(f\"      Fetching page {page} from {cleaned_url.split('?')[0]}...\")\n",
        "\n",
        "         \n",
        "         \n",
        "        page_params = params if page == 1 and '?' not in cleaned_url else None\n",
        "\n",
        "        response = make_api_request(cleaned_url, headers=headers, params=page_params)\n",
        "        page += 1\n",
        "\n",
        "        if not response:\n",
        "            print(\"Failed to fetch paginated data page. Stopping pagination.\")\n",
        "            break  \n",
        "\n",
        "        if response.status_code == 200:\n",
        "            try:\n",
        "                items_page = response.json()\n",
        "                if not items_page:  \n",
        "                    break\n",
        "                if isinstance(items_page, list):\n",
        "                    all_items.extend(items_page)\n",
        "                else:\n",
        "                     \n",
        "                    print(f\"      Warning: Expected a list from {cleaned_url.split('?')[0]}, received type {type(items_page)}. Appending.\")\n",
        "                    all_items.append(items_page)  \n",
        "\n",
        "                 \n",
        "                if 'next' in response.links:\n",
        "                    current_url = response.links['next']['url']\n",
        "                    params = None  \n",
        "                else:\n",
        "                    current_url = None  \n",
        "            except json.JSONDecodeError as e:\n",
        "                print(f\"      Error decoding JSON from {cleaned_url.split('?')[0] if cleaned_url else url}: {e}\")\n",
        "                print(f\"      Response text: {response.text[:200]}...\")  \n",
        "                break  \n",
        "            except Exception as e:\n",
        "                 print(f\"      An unexpected error occurred processing page {page-1} data: {e}\")\n",
        "                 break  \n",
        "\n",
        "        else:  \n",
        "            print(f\"Stopping pagination. Received status {response.status_code} for page {page-1}.\")\n",
        "            break  \n",
        "\n",
        "    return all_items\n",
        "\n",
        "def get_file_content(owner, repo, file_path, commit_sha, headers):\n",
        "    api_url = f\"https://api.github.com/repos/{owner}/{repo}/contents/{file_path}?ref={commit_sha}\"\n",
        "    print(f\"        Fetching content: {file_path} @ {commit_sha[:7]}\")\n",
        "    response = make_api_request(api_url, headers=headers)\n",
        "\n",
        "    if response and response.status_code == 200:\n",
        "        try:\n",
        "            content_data = response.json()\n",
        "            if isinstance(content_data, dict) and content_data.get('type') == 'file' and 'content' in content_data:\n",
        "                if content_data.get('encoding') == 'base64':\n",
        "                    try:\n",
        "                        encoded_content = content_data['content'].replace('\\n', '')\n",
        "                        decoded_content = base64.b64decode(encoded_content).decode('utf-8', errors='replace')\n",
        "                        return decoded_content\n",
        "                    except Exception as e:\n",
        "                        print(f\"        Error decoding base64 content for {file_path} @ {commit_sha[:7]}: {e}\")\n",
        "                        return \"\"  \n",
        "                else:\n",
        "                      \n",
        "                     print(f\"        Warning: Content for {file_path} @ {commit_sha[:7]} not base64 encoded, returning raw.\")\n",
        "                     return content_data['content']\n",
        "            elif isinstance(content_data, dict) and content_data.get('type') in ['dir', 'submodule', 'symlink']:\n",
        "                 print(f\"        Skipping content fetch for non-file type '{content_data.get('type')}' for {file_path} @ {commit_sha[:7]}.\")\n",
        "                 return \"\"  \n",
        "            else:\n",
        "                print(f\"        Warning: Could not get file content (unexpected format or missing content) for {file_path} @ {commit_sha[:7]}. Response type: {type(content_data)}. Content type: {content_data.get('type') if isinstance(content_data, dict) else 'N/A'}\")\n",
        "                return \"\"  \n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"        Error decoding JSON response for file content {file_path} @ {commit_sha[:7]}: {e}\")\n",
        "            return \"\"  \n",
        "    elif response and response.status_code == 404:\n",
        "        print(f\"        File not found (404): {file_path} @ {commit_sha[:7]}\")\n",
        "        return \"\"  \n",
        "    else:\n",
        "         \n",
        "        return \"\"\n",
        "\n",
        "\n",
        "def save_file(content, base_dir, relative_path):\n",
        "\n",
        "    if content is None or content == \"\":  \n",
        "         \n",
        "        return False\n",
        "    try:\n",
        "         \n",
        "        full_path = os.path.join(base_dir, relative_path)\n",
        "\n",
        "         \n",
        "        parent_dir = os.path.dirname(full_path)\n",
        "        if parent_dir:  \n",
        "            os.makedirs(parent_dir, exist_ok=True)\n",
        "\n",
        "         \n",
        "        with open(full_path, 'w', encoding='utf-8') as f:\n",
        "            f.write(content)\n",
        "         \n",
        "        return True\n",
        "    except IOError as e:\n",
        "        print(f\"          Error writing file {full_path}: {e}\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(f\"          Unexpected error saving file {full_path}: {e}\")\n",
        "        return False\n",
        "\n",
        "\n",
        "def process_pull_request_files(owner, repo, pr_number, base_sha, head_sha, headers, pr_output_dir):\n",
        "\n",
        "    api_url = f\"https://api.github.com/repos/{owner}/{repo}/pulls/{pr_number}/files\"\n",
        "    files_list = fetch_paginated_data(api_url, headers=headers)\n",
        "\n",
        "    if not files_list:\n",
        "        print(f\"    No files found or error fetching files for PR \") \n",
        "        return []  \n",
        "\n",
        "    print(f\"    Processing {len(files_list)} files for PR  \")\n",
        "\n",
        "     \n",
        "    before_dir = os.path.join(pr_output_dir, \"before_merge\")\n",
        "    after_dir = os.path.join(pr_output_dir, \"after_merge\")\n",
        "    patch_dir = os.path.join(pr_output_dir, \"changed_files\")\n",
        "    os.makedirs(before_dir, exist_ok=True)\n",
        "    os.makedirs(after_dir, exist_ok=True)\n",
        "    os.makedirs(patch_dir, exist_ok=True)\n",
        "\n",
        "    processed_files_metadata = []\n",
        "    for f in files_list:\n",
        "         \n",
        "        if not isinstance(f, dict):\n",
        "             continue\n",
        "\n",
        "        filename = f.get('filename')\n",
        "        status = f.get('status')\n",
        "\n",
        "        if not filename or not status:\n",
        "             continue\n",
        "\n",
        "        print(f\"      Processing file: {filename} (Status: {status})\")\n",
        "\n",
        "         \n",
        "        content_base = \"\"  \n",
        "         \n",
        "        if status != 'added' and base_sha:\n",
        "            content_base = get_file_content(owner, repo, filename, base_sha, headers)\n",
        "            if content_base:  \n",
        "                 save_file(content_base, before_dir, filename)\n",
        "\n",
        "         \n",
        "        content_head = \"\"  \n",
        "         \n",
        "         \n",
        "        if status not in ['removed', 'deleted'] and head_sha:\n",
        "            content_head = get_file_content(owner, repo, filename, head_sha, headers)\n",
        "            if content_head:  \n",
        "                save_file(content_head, after_dir, filename)\n",
        "\n",
        "         \n",
        "        patch_content = f.get('patch')\n",
        "        if patch_content:\n",
        "            patch_filename = filename + \".patch\"\n",
        "            save_file(patch_content, patch_dir, patch_filename)\n",
        "\n",
        "         \n",
        "        processed_files_metadata.append({\n",
        "            'filename': filename,\n",
        "            'status': status,\n",
        "            'additions': f.get('additions', 0),  \n",
        "            'deletions': f.get('deletions', 0),  \n",
        "            'changes': f.get('changes', 0),      \n",
        "            'sha': f.get('sha'),  \n",
        "            'blob_url': f.get('blob_url'),\n",
        "            'raw_url': f.get('raw_url'),\n",
        "            'patch_saved': bool(patch_content),  \n",
        "            'content_base_saved': bool(content_base),  \n",
        "            'content_head_saved': bool(content_head),  \n",
        "            'previous_filename': f.get('previous_filename')  \n",
        "        })\n",
        "\n",
        "    return processed_files_metadata\n",
        "\n",
        "\n",
        "def get_pr_reviews(owner, repo, pr_number, headers):\n",
        "    \"\"\"Fetches all reviews for a PR. Returns a list of simplified review dictionaries.\"\"\"\n",
        "    api_url = f\"https://api.github.com/repos/{owner}/{repo}/pulls/{pr_number}/reviews\"\n",
        "    reviews = fetch_paginated_data(api_url, headers=headers)\n",
        "    if not reviews: return []\n",
        "    return [\n",
        "        {\n",
        "            'id': r.get('id'),\n",
        "            'user': r.get('user', {}).get('login', 'ghost'),  \n",
        "            'state': r.get('state'),\n",
        "            'submitted_at': r.get('submitted_at'),\n",
        "            'body': r.get('body'),  \n",
        "            'commit_id': r.get('commit_id')\n",
        "        } for r in reviews if isinstance(r, dict)  \n",
        "    ]\n",
        "\n",
        "def get_pr_review_comments(owner, repo, pr_number, headers):\n",
        "    \"\"\"Fetches all review comments (inline code comments) for a PR. Returns a list of simplified comment dictionaries.\"\"\"\n",
        "    api_url = f\"https://api.github.com/repos/{owner}/{repo}/pulls/{pr_number}/comments\"\n",
        "    comments = fetch_paginated_data(api_url, headers=headers)\n",
        "    if not comments: return []\n",
        "    return [\n",
        "        {\n",
        "            'id': c.get('id'),\n",
        "            'user': c.get('user', {}).get('login', 'ghost'),  \n",
        "            'body': c.get('body'),  \n",
        "            'path': c.get('path'),  \n",
        "            'position': c.get('position'),  \n",
        "            'original_position': c.get('original_position'),\n",
        "            'commit_id': c.get('commit_id'),  \n",
        "            'original_commit_id': c.get('original_commit_id'),\n",
        "            'created_at': c.get('created_at'),\n",
        "            'updated_at': c.get('updated_at'),\n",
        "            'in_reply_to_id': c.get('in_reply_to_id')  \n",
        "        } for c in comments if isinstance(c, dict)  \n",
        "    ]\n",
        "\n",
        "def get_pr_issue_comments(owner, repo, pr_number, headers):\n",
        "    \"\"\"Fetches all general issue comments (comments on the PR itself) for a PR. Returns a list of simplified comment dictionaries.\"\"\"\n",
        "    api_url = f\"https://api.github.com/repos/{owner}/{repo}/issues/{pr_number}/comments\"\n",
        "    comments = fetch_paginated_data(api_url, headers=headers)\n",
        "    if not comments: return []\n",
        "    return [\n",
        "        {\n",
        "            'id': c.get('id'),\n",
        "            'user': c.get('user', {}).get('login', 'ghost'),  \n",
        "            'body': c.get('body'),  \n",
        "            'created_at': c.get('created_at'),\n",
        "            'updated_at': c.get('updated_at')\n",
        "        } for c in comments if isinstance(c, dict)  \n",
        "    ]\n",
        "\n",
        "def get_pr_commits(owner, repo, pr_number, headers):\n",
        "    \"\"\"Fetches all commits associated with a PR. Returns a list of simplified commit dictionaries.\"\"\"\n",
        "    api_url = f\"https://api.github.com/repos/{owner}/{repo}/pulls/{pr_number}/commits\"\n",
        "    commits = fetch_paginated_data(api_url, headers=headers)\n",
        "    if not commits: return []\n",
        "    return [\n",
        "        {\n",
        "            'sha': c.get('sha'),\n",
        "            'message': c.get('commit', {}).get('message'),  \n",
        "            'author': c.get('commit', {}).get('author'),  \n",
        "            'committer': c.get('commit', {}).get('committer'),  \n",
        "            'api_author_login': c.get('author', {}).get('login') if c.get('author') else None,  \n",
        "            'api_committer_login': c.get('committer', {}).get('login') if c.get('committer') else None,  \n",
        "            'parents': [p.get('sha') for p in c.get('parents', []) if isinstance(p, dict) and p.get('sha')]  \n",
        "        } for c in commits if isinstance(c, dict)  \n",
        "    ]\n",
        "\n",
        "def get_commit_check_runs(owner, repo, ref_sha, headers):\n",
        "    \"\"\"Fetches check runs (newer Checks API) for a specific commit SHA. Returns a list of simplified check run dictionaries.\"\"\"\n",
        "    if not ref_sha: return []\n",
        "    print(f\"    Fetching check runs for commit {ref_sha[:7]}...\")\n",
        "    api_url = f\"https://api.github.com/repos/{owner}/{repo}/commits/{ref_sha}/check-runs\"\n",
        "     \n",
        "    params={'per_page': 100}  \n",
        "    response = make_api_request(api_url, headers=headers, params=params)\n",
        "\n",
        "    if response and response.status_code == 200:\n",
        "        try:\n",
        "            data = response.json()\n",
        "            check_runs_list = []\n",
        "             \n",
        "            if isinstance(data, dict) and 'check_runs' in data and isinstance(data['check_runs'], list):\n",
        "                 check_runs_list = data['check_runs']\n",
        "                  \n",
        "                  \n",
        "            elif isinstance(data, list):  \n",
        "                 check_runs_list = data\n",
        "            else:\n",
        "                 print(f\"    Unexpected response format for check runs for commit {ref_sha[:7]}. Response type: {type(data)}. Keys: {data.keys() if isinstance(data, dict) else 'N/A'}\")\n",
        "                 return []\n",
        "\n",
        "            return [\n",
        "                {\n",
        "                    'name': cr.get('name'),\n",
        "                    'status': cr.get('status'),  \n",
        "                    'conclusion': cr.get('conclusion'),  \n",
        "                    'started_at': cr.get('started_at'),\n",
        "                    'completed_at': cr.get('completed_at'),\n",
        "                    'app_owner': cr.get('app', {}).get('owner', {}).get('login') if isinstance(cr.get('app'), dict) else None,  \n",
        "                    'app_name': cr.get('app', {}).get('name') if isinstance(cr.get('app'), dict) else None  \n",
        "                     \n",
        "                } for cr in check_runs_list if isinstance(cr, dict)  \n",
        "            ]\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"    Error decoding JSON for check runs commit {ref_sha[:7]}: {e}\")\n",
        "            return []\n",
        "        except Exception as e:\n",
        "             print(f\"    An unexpected error occurred processing check runs for commit {ref_sha[:7]}: {e}\")\n",
        "             return []\n",
        "    return []  \n",
        "\n",
        "def get_commit_statuses(owner, repo, ref_sha, headers):\n",
        "    \"\"\"Fetches statuses (older Status API) for a specific commit SHA. Returns a list of simplified status dictionaries.\"\"\"\n",
        "    if not ref_sha: return []\n",
        "    print(f\"    Fetching statuses for commit {ref_sha[:7]}...\")\n",
        "    api_url = f\"https://api.github.com/repos/{owner}/{repo}/commits/{ref_sha}/statuses\"\n",
        "     \n",
        "    statuses = fetch_paginated_data(api_url, headers=headers)  \n",
        "    if not statuses: return []\n",
        "    return [\n",
        "        {\n",
        "            'context': s.get('context'),  \n",
        "            'state': s.get('state'),  \n",
        "            'description': s.get('description'),  \n",
        "            'target_url': s.get('target_url'),  \n",
        "            'creator_login': s.get('creator', {}).get('login') if isinstance(s.get('creator'), dict) else None,  \n",
        "            'created_at': s.get('created_at'),\n",
        "            'updated_at': s.get('updated_at')\n",
        "        } for s in statuses if isinstance(s, dict)  \n",
        "    ]\n",
        "\n",
        "def parse_linked_issues(text):\n",
        "\n",
        "    if not text:\n",
        "        return []\n",
        "     \n",
        "    github_refs_keyword = re.findall(r'(?:close(?:s|d)?|resolve(?:s|d)?|fix(?:es|ed)?)\\s+')\n",
        "     \n",
        "    github_refs_simple = re.findall(r'(?<![a-zA-Z0-9])#(\\d+)\\b', text)\n",
        "     \n",
        "    jira_refs = re.findall(r'\\b([A-Z][A-Z0-9_]+-\\d+)\\b', text)\n",
        "\n",
        "    issues = set()\n",
        "     \n",
        "    for ref in github_refs_keyword: issues.add(f\"GH-{ref}\")\n",
        "     \n",
        "    for ref in github_refs_simple:\n",
        "        if f\"GH-{ref}\" not in issues:\n",
        "             issues.add(f\"GH-{ref}\")\n",
        "     \n",
        "    for ref in jira_refs: issues.add(ref)\n",
        "\n",
        "    return sorted(list(issues))\n",
        "\n",
        " \n",
        "\n",
        "def get_all_pull_requests_structured(owner, repo, state='all'):\n",
        "    if not github_secret:\n",
        "         print(\"Skipping data fetching: GITHUB_BOT_ACCESS_TOKEN is not set.\")\n",
        "         return []\n",
        "\n",
        "    print(f\"--- Starting STRUCTURED pull request data fetch for {owner}/{repo} ---\")\n",
        "    print(f\"--- Output base directory: {OUTPUT_DIR_BASE} ---\")\n",
        "     \n",
        "\n",
        "    os.makedirs(OUTPUT_DIR_BASE, exist_ok=True)  \n",
        "\n",
        "    api_url = f\"https://api.github.com/repos/{owner}/{repo}/pulls\"\n",
        "    headers = {\n",
        "        'Authorization': f'token {github_secret}',\n",
        "        'Accept': 'application/vnd.github.v3+json',  \n",
        "    }\n",
        "    params = {\n",
        "        'state': state,         \n",
        "        'per_page': PER_PAGE,\n",
        "        'sort': 'updated',      \n",
        "        'direction': 'desc',    \n",
        "        'page': 1               \n",
        "    }\n",
        "     \n",
        "\n",
        "    processed_pr_numbers = []  \n",
        "    current_url = api_url\n",
        "    page = 1\n",
        "\n",
        "    while current_url:\n",
        "         \n",
        "        cleaned_url = current_url.replace('[https://', 'https://').replace(']', '')\n",
        "        print(f\"\\nFetching page {page} of pull requests list from {cleaned_url.split('?')[0]}...\")\n",
        "\n",
        "         \n",
        "         \n",
        "        page_params = params if page == 1 and '?' not in cleaned_url else None\n",
        "\n",
        "        response = make_api_request(cleaned_url, headers=headers, params=page_params)\n",
        "        page += 1\n",
        "\n",
        "        if not response:\n",
        "            print(\"Failed to fetch pull requests list page. Stopping.\")\n",
        "            break  \n",
        "\n",
        "        if response.status_code == 200:\n",
        "            try:\n",
        "                pull_requests_page = response.json()\n",
        "                if not pull_requests_page or not isinstance(pull_requests_page, list):\n",
        "                    print(\"No more pull requests found on this page or unexpected format.\")\n",
        "                    break  \n",
        "\n",
        "                print(f\"Processing {len(pull_requests_page)} pull requests from page {page-1}...\")\n",
        "\n",
        "                for pr_summary in pull_requests_page:\n",
        "                     \n",
        "                    if not isinstance(pr_summary, dict):\n",
        "                        print(f\"Skipping unexpected item in PR list (not a dictionary): {pr_summary}\")\n",
        "                        continue\n",
        "\n",
        "                    pr_number = pr_summary.get('number')\n",
        "                    if pr_number is None:\n",
        "                         print(f\"Skipping PR summary with missing number: {pr_summary}\")\n",
        "                         continue\n",
        "\n",
        "                    pr_updated_at = pr_summary.get('updated_at')\n",
        "\n",
        "                     \n",
        "                    pr_output_dir = os.path.join(OUTPUT_DIR_BASE, f\"pr_{pr_number}\")\n",
        "                    os.makedirs(pr_output_dir, exist_ok=True)\n",
        "\n",
        "                     \n",
        "                     \n",
        "                     \n",
        "                    pr_detail_url = pr_summary.get('url')  \n",
        "                    if not pr_detail_url:\n",
        "                        continue\n",
        "\n",
        "                    pr_detail_response = make_api_request(pr_detail_url, headers=headers)\n",
        "                    if not pr_detail_response or pr_detail_response.status_code != 200:\n",
        "                         continue\n",
        "\n",
        "                    try:\n",
        "                        pr = pr_detail_response.json()  \n",
        "                        if not isinstance(pr, dict):\n",
        "                             continue\n",
        "                    except json.JSONDecodeError as e:\n",
        "                        continue\n",
        "\n",
        "                     \n",
        "                    base_sha = pr.get('base', {}).get('sha')  \n",
        "                    head_sha = pr.get('head', {}).get('sha')  \n",
        "                    pr_body = pr.get('body')  \n",
        "\n",
        "                          \n",
        "                          \n",
        "\n",
        "                     \n",
        "                     \n",
        "                    files_metadata = process_pull_request_files(\n",
        "                        owner, repo, pr_number, base_sha, head_sha, headers, pr_output_dir\n",
        "                    )\n",
        "\n",
        "                     \n",
        "                    reviews = get_pr_reviews(owner, repo, pr_number, headers)\n",
        "                    review_comments = get_pr_review_comments(owner, repo, pr_number, headers)\n",
        "                    issue_comments = get_pr_issue_comments(owner, repo, pr_number, headers)\n",
        "                    commits_list = get_pr_commits(owner, repo, pr_number, headers)\n",
        "\n",
        "                    check_runs = []\n",
        "                    statuses = []\n",
        "                    if head_sha:\n",
        "                        check_runs = get_commit_check_runs(owner, repo, head_sha, headers)\n",
        "                         \n",
        "                        statuses = get_commit_statuses(owner, repo, head_sha, headers)\n",
        "                    \n",
        "                     \n",
        "                    linked_issues = set()\n",
        "                    if pr_body:  \n",
        "                        linked_issues.update(parse_linked_issues(pr_body))\n",
        "                    for c in commits_list:\n",
        "                         \n",
        "                        if isinstance(c, dict) and c.get('message'):\n",
        "                            linked_issues.update(parse_linked_issues(c.get('message')))\n",
        "                     \n",
        "                    for ic in issue_comments:\n",
        "                        if isinstance(ic, dict) and ic.get('body'):\n",
        "                            linked_issues.update(parse_linked_issues(ic.get('body')))\n",
        "                    for r in reviews:\n",
        "                         if isinstance(r, dict) and r.get('body'):\n",
        "                            linked_issues.update(parse_linked_issues(r.get('body')))\n",
        "                    for rc in review_comments:\n",
        "                         if isinstance(rc, dict) and rc.get('body'):\n",
        "                            linked_issues.update(parse_linked_issues(rc.get('body')))\n",
        "\n",
        "\n",
        "                     \n",
        "                    metadata = {\n",
        "                        'pr_number': pr_number,\n",
        "                        'api_url': pr.get('url'),\n",
        "                        'html_url': pr.get('html_url'),\n",
        "                        'state': pr.get('state'),\n",
        "                        'title': pr.get('title'),\n",
        "                        'author_login': pr.get('user', {}).get('login', 'ghost') if isinstance(pr.get('user'), dict) else 'ghost',  \n",
        "                        'author_association': pr.get('author_association'),\n",
        "                        'body': pr_body,  \n",
        "                        'created_at': pr.get('created_at'),\n",
        "                        'updated_at': pr.get('updated_at'),\n",
        "                        'closed_at': pr.get('closed_at'),\n",
        "                        'merged_at': pr.get('merged_at'),\n",
        "                        'merge_commit_sha': pr.get('merge_commit_sha'),\n",
        "                        'assignee': pr.get('assignee', {}).get('login') if isinstance(pr.get('assignee'), dict) else None,  \n",
        "                        'assignees': [a.get('login') for a in pr.get('assignees', []) if isinstance(a, dict) and a.get('login')],  \n",
        "                        'requested_reviewers': [rr.get('login') for rr in pr.get('requested_reviewers', []) if isinstance(rr, dict) and rr.get('login')],  \n",
        "                        'requested_teams': [rt.get('slug') for rt in pr.get('requested_teams', []) if isinstance(rt, dict) and rt.get('slug')],  \n",
        "                        'labels': [l.get('name') for l in pr.get('labels', []) if isinstance(l, dict) and l.get('name')],  \n",
        "                        'is_draft': pr.get('draft', False),\n",
        "                        'merged': pr.get('merged', False),\n",
        "                        'mergeable': pr.get('mergeable'),  \n",
        "                        'mergeable_state': pr.get('mergeable_state'),  \n",
        "                        'merged_by_login': pr.get('merged_by', {}).get('login') if isinstance(pr.get('merged_by'), dict) else None,  \n",
        "                        'base_branch': pr.get('base', {}).get('ref') if isinstance(pr.get('base'), dict) else None,  \n",
        "                        'base_commit_sha': base_sha,\n",
        "                        'head_branch': pr.get('head', {}).get('ref') if isinstance(pr.get('head'), dict) else None,  \n",
        "                        'head_repo_full_name': pr.get('head', {}).get('repo', {}).get('full_name') if isinstance(pr.get('head'), dict) and isinstance(pr.get('head').get('repo'), dict) else None,  \n",
        "                        'head_commit_sha': head_sha,\n",
        "                        'reviews': reviews,\n",
        "                        'review_comments': review_comments,\n",
        "                        'issue_comments': issue_comments,\n",
        "                        'commits_list': commits_list,\n",
        "                        'commits_count': len(commits_list),  \n",
        "                        'check_runs': check_runs,  \n",
        "                        'statuses': statuses,      \n",
        "                        'linked_issues_parsed': sorted(list(linked_issues)),\n",
        "                        'changed_files_count': len(files_metadata),  \n",
        "                        'total_additions': sum(f.get('additions', 0) for f in files_metadata),  \n",
        "                        'total_deletions': sum(f.get('deletions', 0) for f in files_metadata),  \n",
        "                        'changed_files_manifest': files_metadata  \n",
        "                    }\n",
        "\n",
        "                     \n",
        "                    metadata_filename = os.path.join(pr_output_dir, \"metadata.json\")\n",
        "                    try:\n",
        "                        with open(metadata_filename, 'w', encoding='utf-8') as f:\n",
        "                             \n",
        "                            json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
        "                        print(f\"    Successfully saved metadata to {metadata_filename}\")\n",
        "                        processed_pr_numbers.append(pr_number)\n",
        "                    except IOError as e:\n",
        "                        print(f\"    Error writing metadata JSON file {metadata_filename}: {e}\")\n",
        "                    except TypeError as e:\n",
        "                        print(f\"    Error serializing metadata JSON for PR\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"    Unexpected error saving metadata JSON for PR\")\n",
        "\n",
        "                 \n",
        "                if 'next' in response.links:\n",
        "                    current_url = response.links['next']['url']\n",
        "                    params = None  \n",
        "                    print(f\"--- Moving to next page of PR list ---\")\n",
        "                else:\n",
        "                    print(\"No 'next' link found in PR list response, reached the last page.\")\n",
        "                    current_url = None  \n",
        "\n",
        "            except json.JSONDecodeError as e:\n",
        "                print(f\"Error decoding JSON from PR list page {page-1}: {e}\")\n",
        "                print(f\"Response text: {response.text[:200]}...\")  \n",
        "                break  \n",
        "            except Exception as e:\n",
        "                 print(f\"An unexpected error occurred while processing PRs on page {page-1}: {e}\")\n",
        "                 break  \n",
        "\n",
        "        else:  \n",
        "            print(f\"Stopping pagination. Received status {response.status_code} for PR list page {page-1}.\")\n",
        "            break  \n",
        "\n",
        "    print(f\"\\n--- Finished STRUCTURED processing for {owner}/{repo}. ---\")\n",
        "    print(f\"--- Processed {len(processed_pr_numbers)} pull requests. ---\")\n",
        "    print(f\"--- Data saved in subdirectories within: {OUTPUT_DIR_BASE} ---\")\n",
        "    return processed_pr_numbers  \n",
        "\n",
        "\n",
        " \n",
        "class PRSpecificRAG:\n",
        "\n",
        "    def __init__(self, data_path=\"pull_request_data_structured\"):\n",
        "\n",
        "        self.data_path = data_path\n",
        "         \n",
        "        print(\"Initializing embeddings model...\")\n",
        "        try:\n",
        "            self.embeddings = HuggingFaceEmbeddings(\n",
        "                model_name=\"microsoft/graphcodebert-base\",\n",
        "                model_kwargs={\"trust_remote_code\": True}  \n",
        "            )\n",
        "            print(\"Embeddings model initialized.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error initializing embeddings model: {str(e)}\")\n",
        "            self.embeddings = None  \n",
        "             \n",
        "             \n",
        "\n",
        "\n",
        "         \n",
        "        self.splitter = RecursiveCharacterTextSplitter.from_language(\n",
        "            language=Language.PYTHON,\n",
        "            chunk_size=2048,  \n",
        "            chunk_overlap=50  \n",
        "        )\n",
        "        self.pr_databases = {}  \n",
        "        self.llm = None  \n",
        "\n",
        "         \n",
        "        self._temp_chroma_dirs = {}\n",
        "\n",
        "    def __del__(self):\n",
        "\n",
        "        print(\"Cleaning up temporary Chroma directories...\")\n",
        "        for pr_number, temp_dir in self._temp_chroma_dirs.items():\n",
        "            try:\n",
        "                if os.path.exists(temp_dir):\n",
        "                    shutil.rmtree(temp_dir)\n",
        "                    print(f\"Cleaned up temporary directory for PR {pr_number}: {temp_dir}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error cleaning up temporary directory {temp_dir} for PR {pr_number}: {e}\")\n",
        "\n",
        "\n",
        "    def _load_pr_metadata(self, pr_dir):\n",
        "\n",
        "        metadata_path = os.path.join(pr_dir, \"metadata.json\")\n",
        "        if not os.path.exists(metadata_path):\n",
        "            raise FileNotFoundError(f\"Metadata file not found for PR in {pr_dir}\")\n",
        "        with open(metadata_path, \"r\", encoding='utf-8') as f:\n",
        "            return json.load(f)\n",
        "\n",
        "    def _process_single_pr(self, pr_dir_name):\n",
        "\n",
        "        pr_number_str = pr_dir_name.split(\"_\")[-1]  \n",
        "        try:\n",
        "            pr_number = int(pr_number_str)\n",
        "        except ValueError:\n",
        "            print(f\"Warning: Could not parse PR number from directory name: {pr_dir_name}. Skipping.\")\n",
        "            return None  \n",
        "\n",
        "        full_path = os.path.join(self.data_path, pr_dir_name)\n",
        "        if not os.path.isdir(full_path):\n",
        "             print(f\"Warning: PR directory not found or is not a directory: {full_path}. Skipping.\")\n",
        "             return None  \n",
        "\n",
        "\n",
        "        try:\n",
        "             \n",
        "            metadata = self._load_pr_metadata(full_path)\n",
        "        except FileNotFoundError as e:\n",
        "            print(f\"Error loading metadata for PR {pr_number_str}: {e}. Skipping.\")\n",
        "            return None\n",
        "        except json.JSONDecodeError as e:\n",
        "             print(f\"Error decoding metadata JSON for PR {pr_number_str}: {e}. Skipping.\")\n",
        "             return None\n",
        "        except Exception as e:\n",
        "             print(f\"An unexpected error occurred loading metadata for PR {pr_number_str}: {e}. Skipping.\")\n",
        "             return None\n",
        "\n",
        "        if self.embeddings is None:\n",
        "             print(f\"Skipping vector DB creation for PR {pr_number_str}: Embeddings model not initialized.\")\n",
        "             return None\n",
        "\n",
        "        chunks = []\n",
        "         \n",
        "        changed_files = metadata.get(\"changed_files_manifest\", [])\n",
        "        if not changed_files:\n",
        "             print(f\"No changed files found in metadata for PR {pr_number_str}.\")\n",
        "             pass  \n",
        "\n",
        "        for file_meta in tqdm(changed_files, desc=f\"Processing files for PR {pr_number_str}\"):\n",
        "             \n",
        "            if not isinstance(file_meta, dict) or 'filename' not in file_meta:\n",
        "                 print(f\"Warning: Skipping invalid file metadata entry for PR {pr_number_str}: {file_meta}\")\n",
        "                 continue\n",
        "\n",
        "            filename = file_meta[\"filename\"]\n",
        "            try:\n",
        "                 \n",
        "                before_code = self._read_code_file(full_path, \"before_merge\", filename)\n",
        "                after_code = self._read_code_file(full_path, \"after_merge\", filename)\n",
        "                patch = self._read_patch_file(full_path, filename)\n",
        "\n",
        "                 \n",
        "                context = self._create_context(metadata, filename, before_code, after_code, patch)\n",
        "\n",
        "                 \n",
        "                file_chunks = self.splitter.split_text(context)\n",
        "\n",
        "                 \n",
        "                chunks.extend(file_chunks)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing file {filename} in PR {pr_number_str}: {str(e)}\")\n",
        "\n",
        "         \n",
        "         \n",
        "        pr_body = metadata.get(\"body\")\n",
        "        if pr_body:\n",
        "             body_chunks = self.splitter.split_text(f\"PR Body:\\n{pr_body}\")\n",
        "             chunks.extend(body_chunks)\n",
        "\n",
        "\n",
        "        issue_comments = metadata.get(\"issue_comments\", [])\n",
        "        for comment in issue_comments:\n",
        "             if isinstance(comment, dict) and comment.get(\"body\"):\n",
        "                  comment_chunks = self.splitter.split_text(f\"Issue Comment by {comment.get('user', 'N/A')}:\\n{comment.get('body')}\")\n",
        "                  chunks.extend(comment_chunks)\n",
        "\n",
        "\n",
        "        review_comments = metadata.get(\"review_comments\", [])\n",
        "        for comment in review_comments:\n",
        "             if isinstance(comment, dict) and comment.get(\"body\"):\n",
        "                  comment_chunks = self.splitter.split_text(f\"Review Comment by {comment.get('user', 'N/A')} on {comment.get('path', 'N/A')}:\\n{comment.get('body')}\")\n",
        "                  chunks.extend(comment_chunks)\n",
        "\n",
        "\n",
        "        if not chunks:\n",
        "            print(f\"No processable content found for PR {pr_number_str}. Skipping vector DB creation.\")\n",
        "            return None  \n",
        "\n",
        "         \n",
        "         \n",
        "        try:\n",
        "            temp_dir = tempfile.mkdtemp(prefix=f\"chroma_db_pr_{pr_number_str}_\")\n",
        "            self._temp_chroma_dirs[pr_number_str] = temp_dir  \n",
        "\n",
        "            vector_db = Chroma.from_texts(\n",
        "                texts=chunks,\n",
        "                embedding=self.embeddings,\n",
        "                persist_directory=temp_dir  \n",
        "            )\n",
        "            print(f\"Successfully created Chroma DB for PR {pr_number_str} in temporary directory: {temp_dir}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error creating Chroma DB for PR {pr_number_str}: {str(e)}\")\n",
        "             \n",
        "            if pr_number_str in self._temp_chroma_dirs:\n",
        "                 try:\n",
        "                     shutil.rmtree(self._temp_chroma_dirs[pr_number_str])\n",
        "                     del self._temp_chroma_dirs[pr_number_str]\n",
        "                 except Exception as cleanup_e:\n",
        "                     print(f\"Error during cleanup of temp dir {temp_dir}: {cleanup_e}\")\n",
        "\n",
        "            return None  \n",
        "\n",
        "         \n",
        "        self.pr_databases[pr_number_str] = vector_db  \n",
        "        return vector_db\n",
        "\n",
        "    def _read_code_file(self, pr_path, dir_name, filename):\n",
        "\n",
        "        file_path = os.path.join(pr_path, dir_name, filename)\n",
        "        if os.path.exists(file_path):\n",
        "            try:\n",
        "                with open(file_path, \"r\", encoding='utf-8') as f:\n",
        "                    return f.read()\n",
        "            except Exception as e:\n",
        "                print(f\"Error reading file {file_path}: {str(e)}\")\n",
        "                return \"\"  \n",
        "        return \"\"  \n",
        "\n",
        "    def _read_patch_file(self, pr_path, filename):\n",
        "\n",
        "        patch_path = os.path.join(pr_path, \"changed_files\", filename + \".patch\")\n",
        "        if os.path.exists(patch_path):\n",
        "            try:\n",
        "                with open(patch_path, \"r\", encoding='utf-8') as f:\n",
        "                    return f.read()\n",
        "            except Exception as e:\n",
        "                print(f\"Error reading patch file {patch_path}: {str(e)}\")\n",
        "                return \"\"  \n",
        "        return \"\"  \n",
        "\n",
        "    def _create_context(self, metadata, filename, before_code, after_code, patch):\n",
        "         \n",
        "        file_review_comments = [\n",
        "            c.get('body') for c in metadata.get('review_comments', [])\n",
        "            if isinstance(c, dict) and c.get('path') == filename and c.get('body')  \n",
        "        ]\n",
        "        comments_text = \"\\n\".join(file_review_comments) if file_review_comments else \"No specific review comments for this file.\"\n",
        "\n",
        "         \n",
        "        ci_checks_list = [c.get('name', 'N/A') for c in metadata.get('check_runs', []) if isinstance(c, dict)]\n",
        "        ci_checks_str = \", \".join(ci_checks_list) if ci_checks_list else \"No CI checks found.\"\n",
        "\n",
        "\n",
        "        context = (\n",
        "            f\"--- Pull Request {metadata.get('pr_number', 'N/A')} - {metadata.get('title', 'N/A')} ---\\n\"\n",
        "            f\"Author: {metadata.get('author_login', 'N/A')}\\n\"\n",
        "            f\"File: {filename}\\n\"\n",
        "            f\"Status: {metadata.get('state', 'N/A')}\\n\"\n",
        "            f\"CI Checks for head commit: {ci_checks_str}\\n\\n\"\n",
        "            f\"BEFORE CODE:\\n{before_code}\\n\\n\"\n",
        "            f\"AFTER CODE:\\n{after_code}\\n\\n\"\n",
        "            f\"DIFF:\\n{patch}\\n\\n\"\n",
        "            f\"REVIEW COMMENTS on this file:\\n{comments_text}\\n\"\n",
        "        )\n",
        "        return context\n",
        "\n",
        "    def initialize_llm(self):\n",
        "\n",
        "        if self.llm is not None:\n",
        "            print(\"LLM already initialized.\")\n",
        "            return\n",
        "\n",
        "         \n",
        "        model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  \n",
        "        print(f\"Initializing LLM: {model_name}...\")\n",
        "        try:\n",
        "            tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "            model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "            text_gen_pipeline = pipeline(\n",
        "                \"text-generation\",\n",
        "                model=model,\n",
        "                tokenizer=tokenizer,\n",
        "                temperature=0.1,  \n",
        "                max_new_tokens=512,  \n",
        "                repetition_penalty=1.1  \n",
        "                 \n",
        "            )\n",
        "\n",
        "            self.llm = HuggingFacePipeline(pipeline=text_gen_pipeline)\n",
        "            print(\"LLM initialized successfully.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error initializing LLM: {str(e)}\")\n",
        "            self.llm = None  \n",
        "             \n",
        "             \n",
        "\n",
        "\n",
        "    def load_pr(self, pr_number):\n",
        "\n",
        "        pr_number_str = str(pr_number)  \n",
        "        pr_dir_name = f\"pr_{pr_number_str}\"  \n",
        "        pr_dir_path = os.path.join(self.data_path, pr_dir_name)\n",
        "\n",
        "        if not os.path.exists(pr_dir_path):\n",
        "            print(f\"Error: Data directory for PR {pr_number_str} not found at {pr_dir_path}\")\n",
        "            return None\n",
        "\n",
        "         \n",
        "         \n",
        "        vector_db = self._process_single_pr(pr_dir_name)\n",
        "        return vector_db  \n",
        "\n",
        "    def get_review(self, pr_number, question):\n",
        "\n",
        "        pr_number_str = str(pr_number)  \n",
        "\n",
        "        try:\n",
        "            if self.llm is None:\n",
        "                 raise ValueError(\"LLM is not initialized. Call initialize_llm() first.\")\n",
        "\n",
        "             \n",
        "            if pr_number_str not in self.pr_databases or self.pr_databases[pr_number_str] is None:\n",
        "                print(f\"PR {pr_number_str} data not loaded. Attempting to load...\")\n",
        "                loaded_db = self.load_pr(pr_number_str)  \n",
        "                if loaded_db is None:\n",
        "                    raise ValueError(f\"Failed to load data for PR {pr_number_str}. Cannot perform RAG analysis.\")\n",
        "\n",
        "             \n",
        "            retriever = self.pr_databases[pr_number_str].as_retriever(search_kwargs={\"k\": 2})\n",
        "\n",
        "             \n",
        "            source_documents = retriever.get_relevant_documents(question)\n",
        "\n",
        "             \n",
        "            context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc in source_documents])\n",
        "\n",
        "             \n",
        "            pr_dir_name = f\"pr_{pr_number_str}\"\n",
        "            metadata = self._load_pr_metadata(os.path.join(self.data_path, pr_dir_name))\n",
        "            ci_checks_list = [c.get('name', 'N/A') for c in metadata.get('check_runs', []) if isinstance(c, dict)]\n",
        "            ci_checks_str = \", \".join(ci_checks_list) if ci_checks_list else \"No CI checks found.\"\n",
        "\n",
        "             \n",
        "            prompt_template = \"\"\"<|system|>\n",
        "            You are a helpful assistant specializing in code review analysis.\n",
        "            You are analyzing Pull Request {pr_number}. Relevant context from the PR is provided below:\n",
        "\n",
        "            {context}\n",
        "\n",
        "            Consider the following aspects from the PR data:\n",
        "            1. Code changes (diff)\n",
        "            2. Developer comments (issue and review comments)\n",
        "            3. Results of CI checks: {ci_checks}\n",
        "            4. Commit history (summarized in context)\n",
        "\n",
        "            Based on the provided context, answer the user's question about the Pull Request.\n",
        "            If the context does not contain enough information to answer the question,\n",
        "            state that you cannot answer based on the available information.\n",
        "            </s>\n",
        "            <|user|>\n",
        "            {question}\n",
        "            </s>\n",
        "            <|assistant|>\n",
        "            \"\"\"\n",
        "\n",
        "             \n",
        "            print(f\"--- Debugging Prompt Variables for PR {pr_number_str} ---\")\n",
        "            print(f\"pr_number_str: {pr_number_str}\")\n",
        "            print(f\"question: {question}\")\n",
        "            print(f\"ci_checks_str: {ci_checks_str}\")\n",
        "            print(f\"context_text (first 200 chars): {context_text[:200]}...\")\n",
        "            print(\"--- End Debugging Print Statements ---\")\n",
        "\n",
        "\n",
        "             \n",
        "            formatted_prompt = prompt_template.format(\n",
        "                pr_number=pr_number_str,\n",
        "                context=context_text,\n",
        "                ci_checks=ci_checks_str,\n",
        "                question=question\n",
        "            )\n",
        "\n",
        "             \n",
        "             \n",
        "            llm_response = self.llm.invoke(formatted_prompt)\n",
        "\n",
        "             \n",
        "             \n",
        "             \n",
        "             \n",
        "            answer = llm_response.split(\"<|assistant|>\")[-1].strip()\n",
        "\n",
        "\n",
        "             \n",
        "            formatted_sources = self._format_sources(source_documents)\n",
        "\n",
        "            return {\n",
        "                \"pr\": pr_number_str,  \n",
        "                \"question\": question,\n",
        "                \"answer\": answer if answer else \"Could not generate an answer based on the available information.\",\n",
        "                \"sources\": formatted_sources\n",
        "            }\n",
        "        except ValueError as e:\n",
        "             \n",
        "            return {\n",
        "                \"pr\": pr_number_str,\n",
        "                \"question\": question,\n",
        "                \"answer\": f\"Error processing PR {pr_number_str}: {e}\",\n",
        "                \"sources\": []\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                \"pr\": pr_number_str,\n",
        "                \"question\": question,\n",
        "                \"answer\": f\"An unexpected error occurred during review generation: {str(e)}\",\n",
        "                \"sources\": []\n",
        "            }\n",
        "\n",
        "\n",
        "    def _format_sources(self, docs):\n",
        "\n",
        "        formatted_sources = []\n",
        "        for doc in docs:\n",
        "             \n",
        "             \n",
        "             \n",
        "            source_info = {\n",
        "                \"content_snippet\": str(doc.page_content)[:200] + \"...\" if doc and hasattr(doc, 'page_content') else \"N/A\",  \n",
        "                \"file\": \"Unknown (metadata not stored)\",  \n",
        "                \"checks\": \"Unknown (metadata not stored)\",\n",
        "                \"author\": \"Unknown (metadata not stored)\"\n",
        "            }\n",
        "            formatted_sources.append(source_info)\n",
        "        return formatted_sources\n",
        "\n",
        "\n",
        " \n",
        "if __name__ == \"__main__\":\n",
        "     \n",
        "     \n",
        "    owner = 'AlfaInsurance'\n",
        "    repo = 'devQ_testData_PythonProject'\n",
        "\n",
        "     \n",
        "     \n",
        "    pr_state = 'all'\n",
        "\n",
        "     \n",
        "     \n",
        "    print(f\"Starting STRUCTURED full pull request data fetch for {owner}/{repo}\")\n",
        "    print(f\"Target PR state: {pr_state}\")\n",
        "    print(f\"Output directory: {OUTPUT_DIR_BASE}\")\n",
        "    print(\"Ensure GITHUB_BOT_ACCESS_TOKEN environment variable is set.\")\n",
        "    print(\"WARNING: This can take a long time and consume significant disk space and API calls.\")\n",
        "\n",
        "    start_time = time.time()\n",
        "     \n",
        "    if github_secret:\n",
        "        processed_prs = get_all_pull_requests_structured(\n",
        "            owner,\n",
        "            repo,\n",
        "            state=pr_state\n",
        "        )\n",
        "    else:\n",
        "        processed_prs = []\n",
        "        print(\"Skipping data fetch because GITHUB_BOT_ACCESS_TOKEN is not set.\")\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    if processed_prs:\n",
        "        print(f\"\\n--------------------------------------------------\")\n",
        "        print(f\"Successfully finished processing.\")\n",
        "        print(f\"Processed {len(processed_prs)} pull requests.\")\n",
        "        print(f\"Data saved in '{OUTPUT_DIR_BASE}' directory, organized by PR number.\")\n",
        "        print(f\"Total execution time: {end_time - start_time:.2f} seconds\")\n",
        "        print(f\"--------------------------------------------------\")\n",
        "    else:\n",
        "        print(\"\\nNo pull requests processed during fetch.\")\n",
        "        print(f\"Total execution time: {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "\n",
        "     \n",
        "    print(\"\\n--- Starting RAG Analysis ---\")\n",
        "    rag_system = PRSpecificRAG()\n",
        "    rag_system.initialize_llm()  \n",
        "\n",
        "     \n",
        "     \n",
        "     \n",
        "     \n",
        "     \n",
        "    pr_numbers_to_analyze = [str(pr_num) for pr_num in processed_prs]  \n",
        "\n",
        "    questions = [\n",
        "        \"Какие потенциальные уязвимости есть в этих изменениях?\",\n",
        "        \"Соответствует ли код стандартам проекта?\",\n",
        "        \"Есть ли проблемы с производительностью в измененном коде?\",\n",
        "        \"Summarize the main changes in PR 1.\",  \n",
        "        \"What were the CI check results for PR 2?\"  \n",
        "    ]\n",
        "\n",
        "    if not pr_numbers_to_analyze:\n",
        "        print(\"No PRs were successfully processed during data fetch. Cannot perform RAG analysis.\")\n",
        "    else:\n",
        "        for pr_num in pr_numbers_to_analyze:\n",
        "            print(f\"\\n\\033[1m--- Analyzing PR {pr_num} ---\\033[0m\")\n",
        "            try:\n",
        "                 \n",
        "                 \n",
        "                 \n",
        "                for q in questions:\n",
        "                     print(f\"\\nВопрос: {q}\")\n",
        "                     result = rag_system.get_review(pr_num, q)\n",
        "                     print(f\"Ответ: {result['answer']}\")\n",
        "                     print(f\"Источники: {result['sources']}\")\n",
        "            except Exception as e:\n",
        "                print(f\"An error occurred during analysis of PR {pr_num}: {str(e)}\")\n",
        "\n",
        "    print(\"\\n--- RAG Analysis Finished ---\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7z6JFo7Rvmse"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
